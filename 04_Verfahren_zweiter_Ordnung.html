
<!DOCTYPE html>

<html lang="de">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>4. Verfahren zweiter Ordnung &#8212; Optimierungsverfahren, Modellierung und Simulation (DSCB410)</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"N": "\\mathbb{N}", "R": "\\mathbb{R}", "B": "\\mathbb{B}", "I": "\\mathbb{BI}", "NN": "\\mathbb{N}", "RR": "\\mathbb{R}", "BB": "\\mathbb{B}", "norm": ["\\left\\lVert#1 \\right\\rVert", 1], "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\begin{pmatrix}"], "emat": ["\\end{pmatrix}"], "bmats": ["\\left(\\begin{smallmatrix}"], "emats": ["\\end{smallmatrix}\\right)"], "scikit": ["\\texttt{scikit-learn}"], "derv": ["\\frac{\\partial #1}{\\partial #2}", 2], "dervquad": ["\\frac{\\partial^2 #1}{\\partial #2^2}", 2], "dervzwei": ["\\frac{\\partial^2 #1}{\\partial {#2} \\partial {#3}}", 3], "v": ["\\mathbf{#1}", 1], "m": ["\\mathbf{#1}", 1], "hyper": ["{\\color{Bittersweet}{#1}}", 1], "initial": "\\DeclareMathOperator{\\initial}{initial}", "reduced": "\\DeclareMathOperator{\\reduced}{reduced}", "lazy": "\\DeclareMathOperator{\\lazy}{lazy}", "ILP": "\\DeclareMathOperator{\\ILP}{ILP}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Stichwortverzeichnis" href="genindex.html" />
    <link rel="search" title="Suche" href="search.html" />
    <link rel="next" title="5. Automatische Differentiation" href="05_Automatische_Differentiation.html" />
    <link rel="prev" title="3. Gradientenverfahren" href="03_Gradientenverfahren.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="de">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/HKA_IWI_Bildmarke-h_RGB.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Optimierungsverfahren, Modellierung und Simulation (DSCB410)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="00_Ueberblick.html">
                    Überblick
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Nichtlineare Optimierung
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_Multivariate_Analysis.html">
   1. Funktionen und Ableitungen
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_Theoretische_Grundlagen.html">
   2. Grundlagen der nichtlinearen Optimierung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_Gradientenverfahren.html">
   3. Gradientenverfahren
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   4. Verfahren zweiter Ordnung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_Automatische_Differentiation.html">
   5. Automatische Differentiation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_Gradientenverfahren_ML.html">
   6. Gradientenverfahren im maschinellen Lernen
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_Grundlagen_beschraenkte_Optimierung.html">
   7. Grundlagen der beschränkten Optimierung
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Lineare Optimierung
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="08_MILP.html">
   8. Gemischt-ganzzahlige lineare Programmierung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_Standardprobleme.html">
   9. Einige Standardprobleme
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_Modellierungstechniken.html">
   10. Fortgeschrittene Modellierungstechniken
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11_Zeitdiskrete_Modelle.html">
   11. Zeitdiskrete Modelle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12_Ablaufplanung.html">
   12. Ablaufplanung
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13_Praxisaspekte.html">
   13. Praxisaspekte beim Lösen von gemischt-ganzzahligen Programmen
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/04_Verfahren_zweiter_Ordnung.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/04_Verfahren_zweiter_Ordnung.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download notebook file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        <a href="_sources/04_Verfahren_zweiter_Ordnung.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#newton-verfahren">
   4.1. Newton Verfahren
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exkurs-konvergenzgeschwindigkeit">
     4.1.1. Exkurs: Konvergenzgeschwindigkeit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#newton-verfahren-fur-quadratische-funktionen">
     4.1.2. Newton Verfahren für quadratische Funktionen
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularisierung-des-verfahrens">
     4.1.3. Regularisierung des Verfahrens
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#newton-verfahren-als-methode-zur-bestimmung-von-nullstellen">
     4.1.4. Newton Verfahren als Methode zur Bestimmung von Nullstellen
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quasi-newton-verfahren">
   4.2. Quasi-Newton Verfahren
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dfp">
     4.2.1. DFP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bfgs">
     4.2.2. BFGS
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#limited-memory-bfgs">
     4.2.3. Limited-memory BFGS
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#zusammenfassung-der-verfahren-erster-und-zweiter-ordnung">
   4.3. Zusammenfassung der Verfahren erster und zweiter Ordnung
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Verfahren zweiter Ordnung</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#newton-verfahren">
   4.1. Newton Verfahren
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exkurs-konvergenzgeschwindigkeit">
     4.1.1. Exkurs: Konvergenzgeschwindigkeit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#newton-verfahren-fur-quadratische-funktionen">
     4.1.2. Newton Verfahren für quadratische Funktionen
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularisierung-des-verfahrens">
     4.1.3. Regularisierung des Verfahrens
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#newton-verfahren-als-methode-zur-bestimmung-von-nullstellen">
     4.1.4. Newton Verfahren als Methode zur Bestimmung von Nullstellen
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quasi-newton-verfahren">
   4.2. Quasi-Newton Verfahren
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dfp">
     4.2.1. DFP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bfgs">
     4.2.2. BFGS
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#limited-memory-bfgs">
     4.2.3. Limited-memory BFGS
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#zusammenfassung-der-verfahren-erster-und-zweiter-ordnung">
   4.3. Zusammenfassung der Verfahren erster und zweiter Ordnung
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="verfahren-zweiter-ordnung">
<span id="sec-newton"></span><h1><span class="section-number">4. </span>Verfahren zweiter Ordnung<a class="headerlink" href="#verfahren-zweiter-ordnung" title="Link zu dieser Überschrift">#</a></h1>
<p>Bei Experimenten mit pathologischen Funktionen wie der Rosenbrock Funktion (siehe Übungen) stellt man fest, dass Gradientenverfahren sehr viele Schritte benötigen. Ein Problem ist, dass die Richtung des steilsten Abstiegs fast orthogonal auf der Richtung steht, die direkt zum Minimum führt, im folgenden Bild etwas unmathematisch “Tal des besten Abstiegs” genannt.</p>
<figure class="align-default" id="fig-rosenbrock">
<img alt="_images/newton_motivation.png" src="_images/newton_motivation.png" />
<figcaption>
<p><span class="caption-number">Abb. 4.1 </span><span class="caption-text">Typisches Verhalten eines Gradientenabstiegs für die Rosenbrock Funktion.</span><a class="headerlink" href="#fig-rosenbrock" title="Link zu diesem Bild">#</a></p>
</figcaption>
</figure>
<p>Im vergangen Kapitel haben wir gesehen, wie man dieses Zick-Zack Verhalten sowie das langsame “Kriechen” des Gradientenabstiegs durch einen Momentumsterm und Normalisierung verbessern kann. Dies funktioniert leider nicht in allen Fällen und es hat außerdem den Nachteil, dass man mit diesen Optionen immer mehr zusätzliche Hyperparameter einführt (Schrittweite, Schrittweitenstrategie, Parameter der Schrittweitenstrategie, Gewicht <span class="math notranslate nohighlight">\(\beta\)</span> des Momentumsterms, Normalisierung), die sorgfältig aufeinander abgestimmt werden müssen. Dies muss man potentiell für jede neue Funktion tun.</p>
<p>Wir schauen uns das Verhalten am Beispiel der Rosenbrock Funktion genauer an. Der Graph der <a class="reference external" href="https://en.wikipedia.org/wiki/Rosenbrock_function">Rosenbrock Funktion</a> (auch <em>Banana Valley Function</em>) ist ein gekrümmtes, steiles Tal, das an den Rändern sehr steil ist und bei dem Tal die Talsohle in einem bananenförmigen Verlauf nur leicht in Richtung des globalen Minimums <span class="math notranslate nohighlight">\((1,1)\)</span> abfällt. Die Funktion ist definiert als:</p>
<div class="math notranslate nohighlight">
\[ 
f(\v x) = (x_1-1)^2 + 100(x_1^2 - x_2)^2
\]</div>
<p>Ihre Hessematrix <span class="math notranslate nohighlight">\(H(\v x)=\nabla^2 f(\v x)\)</span> ist</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H(\v x)=\bmat 2+1200x_1^2-400x_2 &amp; -400x_1\\
      -400x_1 &amp; 200 \emat
\end{split}\]</div>
<p>Wir werten die Hessematrix an einem Punkt in der Nähe der Lösung aus, z.B. <span class="math notranslate nohighlight">\((0.97,0.94)\)</span> (siehe schwarzer Punkt in <a class="reference internal" href="#fig-rosenbrock"><span class="std std-numref">Abb. 4.1</span></a>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
H(0.97,0.94)=\bmat 2+1200 \cdot 0.97^2-400\cdot 0.94 &amp; -400\cdot 0.97\\
      -400\cdot 0.97 &amp; 200 \emat = \bmat 755.08 &amp; -388 \\ -388 &amp; 200\emat
\end{split}\]</div>
<p>Was sagen uns diese Werte? Noch gar nichts – wir erinnern uns, dass die Eigen<em>werte</em> der Hessematrix die Krümmung von <span class="math notranslate nohighlight">\(f\)</span> in Richtung der Eigen<em>vektoren</em> beschreiben. Berechnen wir also Eigenwerte und Eigenvektoren:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Gradient der Rosenbrock Funktion &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="mi">400</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="o">-</span><span class="mi">200</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])])</span>

<span class="k">def</span> <span class="nf">hess</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Hessematrix der Rosenbrock Funktion &quot;&quot;&quot;</span>
    <span class="n">ddf1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="o">+</span><span class="mi">1200</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="o">-</span><span class="mi">400</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">400</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
    <span class="n">ddf2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">400</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">200</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ddf1</span><span class="p">,</span> <span class="n">ddf2</span><span class="p">])</span>

<span class="n">x0</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.97</span><span class="p">,</span><span class="mf">0.94</span><span class="p">]</span>
<span class="n">lam</span><span class="p">,</span> <span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">hess</span><span class="p">(</span><span class="n">x0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Eigenwert 1:</span><span class="si">{</span><span class="n">lam</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">Eigenwert 2:</span><span class="si">{</span><span class="n">lam</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Eigenvektor 1:</span><span class="si">{</span><span class="n">e</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">Eigenvektor 2:</span><span class="si">{</span><span class="n">e</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Richtung des steilsten Anstiegs:</span><span class="si">{</span><span class="n">grad</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">x0</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Eigenwert 1:954.5855445761965
Eigenwert 2:0.4944554238033483
Eigenvektor 1:[ 0.88932258 -0.45728038]
Eigenvektor 2:[0.45728038 0.88932258]
Richtung des steilsten Anstiegs:[ 0.84898637 -0.52841475]
</pre></div>
</div>
</div>
</div>
<p>Wir stellen fest: Die Krümmung in Richtung <span class="math notranslate nohighlight">\(e_1=\bmats 0.89\\-0.46\emats\)</span> ist etwa 1900 Mal so groß wie in Richtung <span class="math notranslate nohighlight">\(e_2=\bmats 0.46\\0.89\emats\)</span>.</p>
<p>Schauen wir uns zum Vergleich die Richtung des steilsten Abstiegs am Punkt <span class="math notranslate nohighlight">\((0.97,0.94)\)</span> an: Der (normalisierte) Gradient von <span class="math notranslate nohighlight">\(f\)</span> ausgewertet am Punkt <span class="math notranslate nohighlight">\((0.97,0.94)\)</span> ist:</p>
<div class="math notranslate nohighlight">
\[ 
\frac{\nabla f(0.97,0.94)}{\norm{\nabla f(0.97,0.94)}_2}= \bmat 0.85,&amp;-0.52 \emat
\]</div>
<p>Wir sehen: die Richtung des steilsten Abstiegs (bzw. Anstiegs, aber das ist ja die gleiche Richtungsvektor nur mit dem Skalar <span class="math notranslate nohighlight">\(-1\)</span> multipliziert) ist der stark gekrümmten Richtung <span class="math notranslate nohighlight">\(e_1\)</span> sehr ähnlich. Das Verfahren macht Schritte in Richtungen sehr starker Krümmung (tatsächlich ist die Krümmung von <span class="math notranslate nohighlight">\(f\)</span> in Richtung des steilsten Anstiegs – oder Abstiegs, das Ergebnis ist dasselbe – gegeben durch <span class="math notranslate nohighlight">\(\nabla f(\v x)H(\v x) \nabla f(\v x)^T\)</span>, hier also <span class="math notranslate nohighlight">\(\bmats 0.85,&amp;-0.52 \emats \bmats 755.08 &amp; -388 \\ -388 &amp; 200\emats \bmats 0.85\\-0.52 \emats=179.6\)</span>).</p>
<p>Welche Konsequenz hat das? Dazu muss man sich vor Augen halten, dass <em>Krümmung</em> die <em>Änderung der Steigung</em> beschreibt. Eine starke Krümmung bedeutet eine große (lokale) Änderungsrate der Steigung. Anschaulich: Geht man ein sehr kleines Stück in eine Richtung, in der die Funktion stark gekrümmt ist, ist dort die Steigung plötzlich ganz anders. Im Falle des Gradientenabstiegs für die Rosenbrock Funktion: Geht man ein Stück in Richtung des steilsten Abstiegs, hat die Funktion dort keinen steilen Abstieg mehr, sondern nur einen flachen Abstieg oder sogar einen Anstieg. Das sagt uns ihre starke Krümmung.</p>
<p>Umgekehrt verspricht die Richtung der schwachen Krümmung, <span class="math notranslate nohighlight">\(e_2\)</span> (entspricht ungefähr dem “Tal des besten Abstiegs” im Bild) zwar lokal nur wenig Abstieg. Da sich die Steigung aber wenig ändert, könnte man durchaus einen größeren Schritt in diese Richtung gehen.</p>
<p>Diese Überlegungen konnten wir mit Hilfe der Hessematrix anstellen. Die Idee von Verfahren zweiter Ordnung ist es, lokale Krümmungsinformation zu benutzen um bessere Richtungen und Schrittweiten zu erhalten. Wir schauen uns im folgenden zwei Varianten dieser Verfahren ein: Das Newton Verfahren und Quasi-Newton Verfahren.</p>
<section id="newton-verfahren">
<h2><span class="section-number">4.1. </span>Newton Verfahren<a class="headerlink" href="#newton-verfahren" title="Link zu dieser Überschrift">#</a></h2>
<p>Der Gradientenabstieg basiert auf einer Taylor-Entwicklung erster Ordnung – einer Linearisierung. Mit Hilfe des Gradienten berechnet man eine lineare Approximation der Funktion und geht auf dieser linearen Funktion ein Stück bergab. Am neuen Punkt linearisiert man wieder usw. Wie weit man jeweils vom Linearisierungspunkt weggeht wird durch die Schrittweite gesteuert. Diese braucht man zwingend, denn da eine lineare Funktion kein Minimum hat, würde es dort ja beliebig lange bergab gehen.</p>
<p>Eine Idee, sich dem Newton Verfahren für Optimierung zu nähern ist folgende: Im Gegensatz zum Gradientenabstieg approximiert man die Funktion lokal nicht durch eine lineare, sondern eine quadratische Funktion. Diese minimiert man und wählt als nächste Iterierte das berechnete Minimum. Dort bildet man wieder eine quadratische Approximation usw.</p>
<p>Die quadratische Approximation erhält man – Sie ahnen es – mit Hilfe des Taylor-Polynoms zweiten Grades berechnet an der Entwicklungsstelle <span class="math notranslate nohighlight">\(\v x^{[k]}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
T(\v x^{[k]} + \v d)=f(\v x^{[k]})+\nabla f(\v x^{[k]})d+\frac{1}{2}\v d^T\v H(\v x^{[k]})\v d
\]</div>
<p>Dies ist eine quadratische Funktion in der Variablen <span class="math notranslate nohighlight">\(\v d\)</span>. Wie wir am Ende von <a class="reference internal" href="02_Theoretische_Grundlagen.html#sec-konvex"><span class="std std-ref">Konvexität</span></a> gesehen haben, hat eine quadratische Funktion ein eindeutiges Minimum, wenn die Hessematrix <span class="math notranslate nohighlight">\(H(\v x^{[k]})\)</span> positiv definit ist (dann ist die Funktion strikt konvex). Weiterhin haben wir in <a class="reference internal" href="02_Theoretische_Grundlagen.html#ex:quadratic">Example 2.5</a> gesehen, dass man den kritischen Punkt einer quadratischen Funktion durch Lösung eines linearen Gleichungssystems bestimmen kann.</p>
<p>Diese Lösung wäre dann der Schritt <span class="math notranslate nohighlight">\(\v d^{[k]}\)</span>, mit dem wir zur nächsten Iterierten <span class="math notranslate nohighlight">\(\v x^{[k+1]}\)</span> kommen.</p>
<p>Wir berechnen den kritischen Punkt des Taylor Polynoms. Es muss gelten <span class="math notranslate nohighlight">\(\nabla T=0\)</span>, also</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla T(\v x^{[k]} + \v d) &amp;= \nabla f(\v x^{[k]})+\v H(\v x^{[k]})\v d=0\\
&amp;=\v H(\v x^{[k]})\v d = -\nabla f(\v x^{[k]})
\end{align*}\]</div>
<p>Falls <span class="math notranslate nohighlight">\(\v H(\v x^{[k]})\)</span> invertierbar ist, ergibt sich die optimale Suchrichtung durch:</p>
<div class="math notranslate nohighlight">
\[
\v d=-\v H(\v x^{[k]})^{-1}\nabla f(\v x^{[k]})
\]</div>
<p>Aber Achtung: dies ist nur für positiv definite <span class="math notranslate nohighlight">\(\v H(\v x^{[k]})\)</span> eine Abstiegsrichtung, denn sonst hätte die quadratische Approximation ja ein Maximum statt ein Minimum.</p>
<p>Insgesamt haben wir also folgenden Algorithmus, der sich gut in <a class="reference internal" href="#alg:gd_allgemein">Algorithm 4.1</a> aus dem letzten Abschnitt einfügt:</p>
<div class="proof algorithm admonition" id="alg:gd_allgemein">
<p class="admonition-title"><span class="caption-number">Algorithm 4.1 </span> (Newton Verfahren zur Optimierung von Funktionen)</p>
<section class="algorithm-content" id="proof-content">
<dl class="simple myst">
<dt>Gegeben:</dt><dd><p>Zwei Mal differenzierbare Funktion <span class="math notranslate nohighlight">\(f:\R^n\rightarrow\R\)</span>.</p>
</dd>
<dt>Gesucht:</dt><dd><p>Lokales Minimum von <span class="math notranslate nohighlight">\(f\)</span>.</p>
</dd>
</dl>
<p><strong>Algorithmus</strong>:</p>
<p>Starte mit initialer Schätzung <span class="math notranslate nohighlight">\(\v x^{[0]}\)</span>, setze <span class="math notranslate nohighlight">\(k=0\)</span>.</p>
<p>Für <span class="math notranslate nohighlight">\(k=0,1,2,\dots\)</span>:</p>
<ol>
<li><p>Überprüfe, ob <span class="math notranslate nohighlight">\(\v x^{[k]}\)</span> die <strong>Abbruchbedingung</strong> erfüllt.</p>
<ul class="simple">
<li><p>Falls ja: Abbruch mit Lösung <span class="math notranslate nohighlight">\(\v x^{[k]}\)</span></p></li>
<li><p>Falls nein: gehe zu Schritt 2.</p></li>
</ul>
</li>
<li><p>Bestimme <strong>Abstiegsrichtung</strong> <span class="math notranslate nohighlight">\(\v d^{[k]}\)</span> durch</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \v d^{[k]}=-\v H(\v x^{[k]})^{-1}\nabla f(\v x^{[k]})^T
    \end{align*}\]</div>
</li>
<li><p><em>Optional:</em> Bestimme Schrittweite <span class="math notranslate nohighlight">\(\alpha^{[k]}\)</span>.</p></li>
<li><p>Berechne neue Iterierte <span class="math notranslate nohighlight">\(\v x^{[k+1]}=\v x^{[k]}+\alpha^{[k]}\v d^{[k]}\)</span>.</p></li>
</ol>
</section>
</div><p>Einige Anmerkungen zu dem Verfahren:</p>
<ol class="simple">
<li><p>Die Inverse der Hessematrix modifiziert nicht nur die Richtung, sondern auch die Schrittlänge geeignet. Oft wird deshalb <span class="math notranslate nohighlight">\(\alpha^{[k]}=1\)</span> gesetzt, d.h. es wird “unsichtbar”.</p></li>
<li><p>Voraussetzung, dass das Verfahren zu einem Minimum konvergiert ist, dass die Hessematrix positiv definit sein muss. Sonst ist nämlich <span class="math notranslate nohighlight">\(\v d^{[k]}\)</span> keine Abstiegsrichtung. Positive Definitheit der Hessematrix ist in der Nähe der Lösung zwar gegeben, aber weit weg von der Lösung (z.B. bei einem x-beliebigen Startwert) ist das nicht immer der Fall.</p></li>
</ol>
<p>Wir schauen uns das Verfahren am Beispiel der Rosenbrock Funktion an.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Function to minimize &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
    
<span class="k">def</span> <span class="nf">df</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Derivative of the function to minimize &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">+</span><span class="mi">400</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="o">-</span><span class="mi">200</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])])</span>

<span class="k">def</span> <span class="nf">ddf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; 2nd Derivative of the function to minimize &quot;&quot;&quot;</span>
    <span class="n">ddf1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="o">+</span><span class="mi">1200</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="o">-</span><span class="mi">400</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">400</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
    <span class="n">ddf2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">400</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">200</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ddf1</span><span class="p">,</span> <span class="n">ddf2</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">newton</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">derv</span><span class="p">,</span> <span class="n">hessian</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">alpha0</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Perform n_steps iterations of newton&#39;s method and return iterates &quot;&quot;&quot;</span>
    <span class="n">x_history</span> <span class="o">=</span> <span class="p">[</span><span class="n">x0</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
        <span class="c1"># Evaluate gradient</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">derv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Stop if norm (length) of gradient vector is small</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-6</span><span class="p">:</span>
            <span class="k">break</span>

        <span class="c1"># Descent direction</span>
        <span class="n">H</span> <span class="o">=</span> <span class="n">hessian</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">d</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">H</span><span class="p">)</span><span class="nd">@g</span>

        <span class="c1"># Step length</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha0</span>

        <span class="c1"># Next iterate</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">d</span>

        <span class="c1"># Store iterate for analysis</span>
        <span class="n">x_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_history</span><span class="p">)</span>

<span class="n">x0</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">newton</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">ddf</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-1.,  1.],
       [ 1., -3.],
       [ 1.,  1.]])
</pre></div>
</div>
</div>
</div>
<p>Wow! Die Funktion, mit der sich das Gradientenverfahren so abgeplagt hat, erledigt das Newton Verfahren in nur wenigen Iterationen. Tatsächlich hat das Newton Verfahren eine starke Eigenschaft, nämlich <em>lokal quadratische</em> Konvergenz.</p>
<section id="exkurs-konvergenzgeschwindigkeit">
<h3><span class="section-number">4.1.1. </span>Exkurs: Konvergenzgeschwindigkeit<a class="headerlink" href="#exkurs-konvergenzgeschwindigkeit" title="Link zu dieser Überschrift">#</a></h3>
<p>Numerische Verfahren, die eine Folge von Iterierten <span class="math notranslate nohighlight">\(\v x^{[k]}\)</span>, <span class="math notranslate nohighlight">\(k=1,2,\dots\)</span> produzieren, kann man anhand ihrer Konvergenzgeschwindigkeit charakterisieren. Die Analyse der Konvergenzgeschwindigkeit ist ein theoretisches Konstrukt und dient dazu, das Verhalten von Verfahren in der Nähe einer (exakten) Lösung <span class="math notranslate nohighlight">\(\v x^*\)</span> zu erklären.</p>
<p>Folgende drei Fälle spielen dabei in der Praxis eine Rolle:</p>
<dl class="simple myst">
<dt>Lineare Konvergenz</dt><dd><p>Ein Verfahren konvergiert (lokal) <strong>linear</strong>, wenn für aufeinanderfolgende Iterierte <span class="math notranslate nohighlight">\(\v x^{[k]}\)</span> und <span class="math notranslate nohighlight">\(\v x^{[k+1]}\)</span> gilt:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \frac{\left\|\v x^{[k+1]}-\v x^*\right\|}{\left\|\v x^{[k]}-\v x^*\right\|}\leq c &lt; 1,\quad \text{für }k\rightarrow \infty
    \end{align*}\]</div>
<p>In Worten: Der Abstand zur Lösung nach dem Schritt ist <span class="math notranslate nohighlight">\(c\cdot100\%\)</span> des Abstandes vor dem Schritt. Der Zugewinn wird also kleiner, je näher man der Lösung kommt. Je kleiner dabei der Wert <span class="math notranslate nohighlight">\(c\)</span> ist, desto besser. Das deckt sich mit unserer empirischen Beobachtung beim Gradientenverfahren. Dieses ist ein linear konvergentes Verfahren.</p>
</dd>
<dt>Quadratische Konvergenz</dt><dd><p>Ein Verfahren konvergiert (lokal) <strong>quadratisch</strong>, wenn für aufeinanderfolgende Iterierte <span class="math notranslate nohighlight">\(\v x^{[k]}\)</span> und <span class="math notranslate nohighlight">\(\v x^{[k+1]}\)</span> gilt:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \frac{\left\|\v x^{[k+1]}-\v x^*\right\|}{\left\|\v x^{[k]}-\v x^*\right\|^2}\leq c &lt; 1,\quad   \text{für }k\rightarrow \infty
    \end{align*}\]</div>
<p>Die Definition ist etwas technisch und ihre Konsequenzen sind möglicherweise nicht direkt klar. Es bedeutet aber in der Praxis, dass sich in der Nähe der Lösung mit jedem Schritt die Anzahl der korrekten Dezimalstellen in etwa <em>verdoppelt</em> – also eine sehr rapide Konvergenz.
Die Konvergenzordnung des Newton Verfahrens ist lokal quadratisch.</p>
</dd>
<dt>Superlineare Konvergenz</dt><dd><p>Ein Verfahren konvergiert (lokal) <strong>superlinear</strong>, wenn für aufeinanderfolgende Iterierte <span class="math notranslate nohighlight">\(\v x^{[k]}\)</span> und <span class="math notranslate nohighlight">\(\v x^{[k+1]}\)</span> gilt:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \frac{\left\|\v x^{[k+1]}-\v x^*\right\|}{\left\|\v x^{[k]}-\v x^*\right\|}\leq c_k,
    \end{align*}\]</div>
<p>wobei <span class="math notranslate nohighlight">\(c_k\)</span> eine Folge ist, die gegen <span class="math notranslate nohighlight">\(0\)</span> konvergiert. Ein Verfahren, das superlinear konvergiert, konvergiert schneller als ein lineare konvergentes Verfahren aber in der Regel langsamer als ein quadratisch konvergentes.</p>
</dd>
</dl>
<p>Man sollte noch hervorheben, dass die Konvergenz des Newton Verfahrens <em>lokal</em> quadratisch ist. Lokal bedeutet “in der Nähe” der Lösung, was sich oft nicht einfach charakterisieren lässt. Weiter weg von der Lösung verliert es diese Eigenschaft (auch wenn die Hessematrix positiv definit ist). Man kann das Newton Verfahren z.B. mittels einer Liniensuche global konvergent machen. Das heißt, es konvergiert dann von beliebigen Startwerten aus, aber unter Umständen nicht mehr so schnell.</p>
</section>
<section id="newton-verfahren-fur-quadratische-funktionen">
<h3><span class="section-number">4.1.2. </span>Newton Verfahren für quadratische Funktionen<a class="headerlink" href="#newton-verfahren-fur-quadratische-funktionen" title="Link zu dieser Überschrift">#</a></h3>
<p>Wir haben das Newton Verfahren als lokale Approximation durch ein Taylor-Polynom zweiter Ordnung eingeführt, welches exakt minimiert wird. Das bedeutet aber, dass das Newton Verfahren für eine quadratische Funktion von einem beliebigen Startwert aus konvergiert. Wir rechnen das nach. Sei also</p>
<div class="math notranslate nohighlight">
\[
f(\v x)=\frac{1}{2}\v x^T\v A\v x+\v b^T\v x+c,
\]</div>
<p>eine quadratische Funktion mit positiv definiter Hessematrix <span class="math notranslate nohighlight">\(\v A\)</span>. Dann kann man das Minimum einerseits analytisch bestimmen durch</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\nabla f(\v x)=\v A\v x + \v b=0 \Leftrightarrow \v x=-\v A^{-1}\v b
\end{align*}\]</div>
<p>Andererseits berechnen wir den ersten Schritt des Newton Verfahrens mit beliebigem Startwert <span class="math notranslate nohighlight">\(\v x^{[0]}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\v x^{[1]}=\v x^{[0]}-\v H(\v x^{[0]})^{-1}\nabla f(\v x^{[0]})^T=\v x^{[0]}-\v A^{-1}(\v A\v x^{[0]}+\v b)=-\v A^{-1}\v b
\end{align*}\]</div>
<p>Unabhängig vom Startwert landet das Verfahren in derselben Lösung <span class="math notranslate nohighlight">\(\v A^{-1}\v b\)</span></p>
</section>
<section id="regularisierung-des-verfahrens">
<h3><span class="section-number">4.1.3. </span>Regularisierung des Verfahrens<a class="headerlink" href="#regularisierung-des-verfahrens" title="Link zu dieser Überschrift">#</a></h3>
<p>Wir haben weiter oben schon darauf hingewiesen, dass die Richtung <span class="math notranslate nohighlight">\(H(\v x^{[k]})\nabla f(\v x^{[k]})\)</span>Newton nur für positiv definite <span class="math notranslate nohighlight">\(\v H(\v x^{[k]})\)</span> eine Abstiegsrichtung ist. Falls das nicht der Fall sein sollte, kann die Hessematrix <em>regularisieren</em> um einen Abstieg zu erwzingen. Dafür muss man eine Zahl <span class="math notranslate nohighlight">\(\lambda^{[k]}&gt;0\)</span> bestimmen, so dass</p>
<div class="math notranslate nohighlight">
\[
\v H(\v x^{[k]}) + \lambda^{[k]}\I
\]</div>
<p>positiv definit ist. <span class="math notranslate nohighlight">\(\I\)</span> steht hier für die Einheitsmatrix. Man kann zeigen, dass <span class="math notranslate nohighlight">\(\lambda^{[k]}\)</span> mindestens so groß sein muss wie der Betrag des kleinsten Eigenwertes von <span class="math notranslate nohighlight">\(\v H(\v x^{[k]})\)</span> (da <span class="math notranslate nohighlight">\(\v H(\v x^{[k]})\)</span> nicht positiv definit ist, ist dieser auf jeden Fall negativ).</p>
<p>Der regularisierte Newton Schritt ist dann</p>
<div class="math notranslate nohighlight">
\[
\v x^{[k+1]}=\v x^{[k]} - (\v H(\v x^{[k]}) + \lambda^{[k]}\I)^{-1}\nabla f(\v x^{[k]})^T
\]</div>
<p>Typischerweise wird man versuchen <span class="math notranslate nohighlight">\(\lambda^{[k]}\)</span> so klein wie möglich zu wählen. Es aber interessant festzustellen, dass sich die Abstiegsrichtung des regularisierten Newton Schrittes mit steigendem <span class="math notranslate nohighlight">\(\lambda^{[k]}\)</span> immer mehr der Richtung des steilsten Abstiegs annähert. Allerdings wird der Schritt mit sehr kleiner Schrittweite, proportional zu <span class="math notranslate nohighlight">\(\frac{1}{\lambda^{[k]}}\)</span> ausgeführt.</p>
</section>
<section id="newton-verfahren-als-methode-zur-bestimmung-von-nullstellen">
<h3><span class="section-number">4.1.4. </span>Newton Verfahren als Methode zur Bestimmung von Nullstellen<a class="headerlink" href="#newton-verfahren-als-methode-zur-bestimmung-von-nullstellen" title="Link zu dieser Überschrift">#</a></h3>
<p>Ursrpünglich wurde das Newton Verfahren nicht als Optimierungsverfahren erfunden, sondern als Methode um nichtlineare Gleichungen zu lösen. Das Lösen nichtlinearer Gleichungen ist aber dasselbe wie die Nullstellen von Funktionen zu bestimmen. Nullstellenprobleme sind Probleme der Form <span class="math notranslate nohighlight">\(\v g (\v x)=\v 0\)</span>, wobei <span class="math notranslate nohighlight">\(\v g:\R^n\rightarrow\R^n\)</span> eine vektorwertige Funktion ist (historisch wurden nur Polynomfunktionen untersucht). Die Iterationsvorschrift für das Nullstellenproblem lautet</p>
<div class="math notranslate nohighlight">
\[
\v x^{[k+1]}=\v x^{[k]}-\nabla \v g(\v x^{[k]})^{-1}\v g(\v x^{[k]})
\]</div>
<p>Im Kontext der Optimierung kann man sich das Newton Verfahren als einen Ansatz zum iterativen Lösen der Optimalitätsbedingungen erster Ordnung, <span class="math notranslate nohighlight">\(\nabla f(\v x)=\v 0\)</span> vorstellen (<span class="math notranslate nohighlight">\(\nabla f(\v x)\)</span> spielt dann die Rolle der Funktion <span class="math notranslate nohighlight">\(\v g\)</span>). Das erklärt auch das Verhalten des Newton Verfahrens, möglicherweise zu Maxima oder Sattelpunkten zu konvergieren, da die Optimalitätsbedingungen erster Ordnung zwischen diesen ja auch keinen Unterschied machen. Erklärungen zum klassischen Newton Verfahren für Nullstellen findet man z.B. <a class="reference external" href="https://de.wikipedia.org/wiki/Newtonverfahren">hier</a>.</p>
</section>
</section>
<section id="quasi-newton-verfahren">
<h2><span class="section-number">4.2. </span>Quasi-Newton Verfahren<a class="headerlink" href="#quasi-newton-verfahren" title="Link zu dieser Überschrift">#</a></h2>
<section id="dfp">
<h3><span class="section-number">4.2.1. </span>DFP<a class="headerlink" href="#dfp" title="Link zu dieser Überschrift">#</a></h3>
</section>
<section id="bfgs">
<h3><span class="section-number">4.2.2. </span>BFGS<a class="headerlink" href="#bfgs" title="Link zu dieser Überschrift">#</a></h3>
</section>
<section id="limited-memory-bfgs">
<h3><span class="section-number">4.2.3. </span>Limited-memory BFGS<a class="headerlink" href="#limited-memory-bfgs" title="Link zu dieser Überschrift">#</a></h3>
</section>
</section>
<section id="zusammenfassung-der-verfahren-erster-und-zweiter-ordnung">
<h2><span class="section-number">4.3. </span>Zusammenfassung der Verfahren erster und zweiter Ordnung<a class="headerlink" href="#zusammenfassung-der-verfahren-erster-und-zweiter-ordnung" title="Link zu dieser Überschrift">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="03_Gradientenverfahren.html" title="zurück Seite">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">zurück</p>
            <p class="prev-next-title"><span class="section-number">3. </span>Gradientenverfahren</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="05_Automatische_Differentiation.html" title="weiter Seite">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">weiter</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Automatische Differentiation</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Dennis Janka<br/>
  
      &copy; Copyright 2023.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>